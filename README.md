# ğŸ”§ Korean Tool Calling LoRA Training

Kanana, Qwen ë“± ë‹¤ì–‘í•œ sLLMì— Tool Calling ëŠ¥ë ¥ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ LoRA íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

## ğŸ“Š ë°ì´í„°ì…‹

HuggingFace Hubì—ì„œ ìë™ìœ¼ë¡œ ë¡œë“œë©ë‹ˆë‹¤:

| ë°ì´í„°ì…‹ | ì„¤ëª… | ìƒ˜í”Œ ìˆ˜ |
|---------|------|--------|
| [NotoriousH2/instructkr-toolflow](https://huggingface.co/datasets/NotoriousH2/instructkr-toolflow) | í•œêµ­ì–´ í—¬í”„ë°ìŠ¤í¬ ì‹œë‚˜ë¦¬ì˜¤ | ~1,000 |
| [NotoriousH2/instructkr-when2call](https://huggingface.co/datasets/NotoriousH2/instructkr-when2call) | Tool í˜¸ì¶œ ì—¬ë¶€ íŒë‹¨ | ~15,000 |
| [NotoriousH2/instructkr-apigen](https://huggingface.co/datasets/NotoriousH2/instructkr-apigen) | API ìƒì„± ë©€í‹°í„´ ëŒ€í™” | ~5,000 |

### ë°ì´í„° í˜•ì‹

ë°ì´í„°ì…‹ì€ `messages` + `tools` í˜•ì‹ì´ë©°, í•™ìŠµ ì‹œ ìë™ìœ¼ë¡œ Llama 3 chat template `text`ë¡œ ë³€í™˜ë©ë‹ˆë‹¤:

```json
{
  "messages": [
    {"role": "user", "content": "ì˜¤ëŠ˜ ë‚ ì”¨ ì•Œë ¤ì¤˜"},
    {"role": "assistant", "content": "<function=get_weather>{\"location\": \"ì„œìš¸\"}</function>"}
  ],
  "tools": [
    "{\"name\": \"get_weather\", \"description\": \"ë‚ ì”¨ ì¡°íšŒ\", ...}"
  ]
}
```

â†’ ìë™ ë³€í™˜ â†’

```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You have access to the following functions:
Use the function 'get_weather' to 'ë‚ ì”¨ ì¡°íšŒ'
{"name": "get_weather", ...}
...<|eot_id|><|start_header_id|>user<|end_header_id|>

ì˜¤ëŠ˜ ë‚ ì”¨ ì•Œë ¤ì¤˜<|eot_id|><|start_header_id|>assistant<|end_header_id|>

<function=get_weather>{"location": "ì„œìš¸"}</function><|eot_id|>
```

## ğŸš€ Quick Start

### 1. í™˜ê²½ ì„¤ì •

âš ï¸ **ì„¤ì¹˜ ìˆœì„œê°€ ì¤‘ìš”í•©ë‹ˆë‹¤!** UnslothëŠ” ë°˜ë“œì‹œ ë§ˆì§€ë§‰ì— ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.

```bash
# Step 1: ê¸°ë³¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements_lora.txt

# Step 2: Unsloth ì„¤ì¹˜ (ë°˜ë“œì‹œ ë§ˆì§€ë§‰ì—!)
pip install "unsloth[cu128-torch271] @ git+https://github.com/unslothai/unsloth.git"

# Step 3: Flash Attention ì„¤ì¹˜ (ì„ íƒ, ì„±ëŠ¥ í–¥ìƒ)
pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.9cxx11abiTRUE-cp312-cp312-linux_x86_64.whl
```

> **Note**: UnslothëŠ” transformers, torch ë“±ì„ monkey-patchí•˜ë¯€ë¡œ ë‹¤ë¥¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í›„ì— ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.

### 2. í•™ìŠµ ì‹¤í–‰

```bash
# ê¸°ë³¸ ì‹¤í–‰ (Kanana 2.1B, rank 16)
python train_lora.py

# LoRA Rank ë³€ê²½
python train_lora.py --rank 8
python train_lora.py --rank 32
python train_lora.py --rank 64

# ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©
python train_lora.py --model Qwen/Qwen2.5-3B-Instruct --rank 16
```

### 3. ê²°ê³¼ ì‹œê°í™”

```bash
# ë¡œìŠ¤ ì»¤ë¸Œ ë¹„êµ
python visualize_loss.py experiments/*/training_log.csv

# ì´ë¯¸ì§€ ì €ì¥
python visualize_loss.py -o comparison.png experiments/*/training_log.csv

# ìš”ì•½ í…Œì´ë¸”ë„ ì €ì¥
python visualize_loss.py -o comparison.png -s summary.csv experiments/*/training_log.csv
```

## âš™ï¸ ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… |
|---------|-------|------|
| `--model` | `kakaocorp/kanana-1.5-2.1b-instruct-2505` | HuggingFace ëª¨ë¸ ID |
| `--rank` | 16 | LoRA rank (alphaëŠ” ìë™ìœ¼ë¡œ rankÃ—2) |
| `--lr` | 2e-4 | Learning rate |
| `--epochs` | 3 | í•™ìŠµ ì—í­ ìˆ˜ |
| `--batch_size` | 4 | ë°°ì¹˜ ì‚¬ì´ì¦ˆ |
| `--grad_accum` | 4 | Gradient accumulation steps |
| `--max_seq_length` | 9000 | ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ |

### ì „ì²´ ì˜µì…˜ ë³´ê¸°

```bash
python train_lora.py --help
```

## ğŸ“ ì¶œë ¥ êµ¬ì¡°

```
experiments/
â”œâ”€â”€ kanana_1.5_2.1b_instruct_2505_r16_a32_lr2em04_ep3_20251218_143022/
â”‚   â”œâ”€â”€ experiment_config.json    # ì‹¤í—˜ ì„¤ì •
â”‚   â”œâ”€â”€ training_log.csv          # ë¡œìŠ¤ ë¡œê·¸
â”‚   â”œâ”€â”€ checkpoint-200/           # ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸
â”‚   â”œâ”€â”€ checkpoint-400/
â”‚   â””â”€â”€ final_model/              # ìµœì¢… LoRA ì–´ëŒ‘í„°
â””â”€â”€ ...
```

## ğŸ”¬ ì‹¤í—˜ ì˜ˆì‹œ: LoRA Rank ë¹„êµ

ì—¬ëŸ¬ GPU í´ë¼ìš°ë“œì—ì„œ ë³‘ë ¬ë¡œ ì‹¤í–‰:

```bash
# GPU 1
python train_lora.py --rank 4

# GPU 2
python train_lora.py --rank 8

# GPU 3
python train_lora.py --rank 16

# GPU 4
python train_lora.py --rank 32

# GPU 5
python train_lora.py --rank 64
```

ê²°ê³¼ CSV íŒŒì¼ë“¤ì„ ëª¨ì•„ì„œ ì‹œê°í™”:

```bash
python visualize_loss.py -o rank_comparison.png -s summary.csv experiments/*/training_log.csv
```

## ğŸ“ í•™ìŠµ ë°©ì‹: ìƒŒë“œìœ„ì¹˜ ë§ˆìŠ¤í‚¹

Tool Calling í•™ìŠµì—ì„œ ì¤‘ìš”í•œ ì :
- `system`, `user`, `tool/ipython` ë¸”ë¡ â†’ **ë§ˆìŠ¤í‚¹** (-100)
- `assistant` ë¸”ë¡ â†’ **í•™ìŠµ ëŒ€ìƒ**

ì´ë¥¼ í†µí•´ Tool ê²°ê³¼ë¥¼ í™˜ê°(Hallucination)í•˜ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤.

## ğŸ› ï¸ ìš”êµ¬ì‚¬í•­

- Python 3.12+
- CUDA 12.x
- GPU Memory: 16GB+ ê¶Œì¥

## ğŸ“œ License

MIT License

