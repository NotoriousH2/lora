#!/usr/bin/env python3
"""
Tool Calling ë² ì´ìŠ¤ ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

LoRA í•™ìŠµ ì „ì˜ ë² ì´ìŠ¤ ëª¨ë¸ Tool Calling ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.

í‰ê°€ ì§€í‘œ:
- Tool Selection Accuracy: ì˜¬ë°”ë¥¸ Toolì„ ì„ íƒí–ˆëŠ”ì§€
- Parameter Exact Match: íŒŒë¼ë¯¸í„°ê°€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ”ì§€
- When-to-Call Accuracy: Tool í˜¸ì¶œ ì—¬ë¶€ íŒë‹¨ì´ ë§ëŠ”ì§€
- JSON Parse Success: ìƒì„±ëœ ì‘ë‹µì´ ìœ íš¨í•œ JSONì¸ì§€

ì‚¬ìš©ë²•:
    python evaluate_base.py
    python evaluate_base.py --num_samples 1000
    python evaluate_base.py --base_model kakaocorp/kanana-nano-2.1b-instruct
"""

# ============================================================
# âš ï¸ UnslothëŠ” ë°˜ë“œì‹œ ë‹¤ë¥¸ íŒ¨í‚¤ì§€ë³´ë‹¤ ë¨¼ì € importí•´ì•¼ í•©ë‹ˆë‹¤!
# ============================================================
from unsloth import FastLanguageModel

import argparse
import json
import os
import random
import re
from datetime import datetime
from typing import Optional

import pandas as pd
import torch
from datasets import load_dataset
from tqdm import tqdm


# ============================================================
# HuggingFace Hub ë°ì´í„°ì…‹ ID
# ============================================================
HF_DATASET = "NotoriousH2/instructkr-sft"


# ============================================================
# Tool Call ì¶”ì¶œ í•¨ìˆ˜ (ëª¨ë¸ ì‘ë‹µìš©)
# ============================================================
def extract_tool_call_from_response(response: str) -> Optional[dict]:
    """
    ëª¨ë¸ ì‘ë‹µì—ì„œ Tool Call ì¶”ì¶œ
    
    Returns:
        {"name": "tool_name", "arguments": {...}} ë˜ëŠ” None (Tool Call ì—†ìŒ)
    """
    if not response:
        return None
    
    # í˜•ì‹ 1: <function=name>{"param": "value"}</function>
    pattern1 = r'<function=([^>]+)>(.+?)</function>'
    match1 = re.search(pattern1, response, re.DOTALL)
    if match1:
        tool_name = match1.group(1).strip()
        try:
            arguments = json.loads(match1.group(2).strip())
            return {"name": tool_name, "arguments": arguments}
        except json.JSONDecodeError:
            return {"name": tool_name, "arguments": None, "parse_error": True}
    
    # í˜•ì‹ 2: JSON í˜•ì‹
    try:
        parsed = json.loads(response.strip())
        if isinstance(parsed, dict) and "name" in parsed:
            return {
                "name": parsed.get("name", ""),
                "arguments": parsed.get("arguments", {})
            }
    except json.JSONDecodeError:
        pass
    
    return None


def extract_tool_call_from_data(msg: dict) -> Optional[dict]:
    """
    ë°ì´í„°ì˜ ë©”ì‹œì§€ì—ì„œ Tool Call ì¶”ì¶œ (tool_calls í•„ë“œ ì‚¬ìš©)
    """
    tool_calls = msg.get("tool_calls")
    if not tool_calls or not isinstance(tool_calls, list):
        return None
    
    first_call = tool_calls[0]
    if not isinstance(first_call, dict):
        return None
    
    func_info = first_call.get("function", {})
    if not func_info:
        return None
    
    name = func_info.get("name", "")
    args_str = func_info.get("arguments", "{}")
    
    try:
        if isinstance(args_str, str):
            arguments = json.loads(args_str)
        else:
            arguments = args_str
    except json.JSONDecodeError:
        arguments = {}
    
    return {"name": name, "arguments": arguments}


# ============================================================
# ë¹„êµ í•¨ìˆ˜
# ============================================================
def compare_tool_calls(pred: Optional[dict], gold: Optional[dict], gold_has_tool_call: bool) -> dict:
    """ì˜ˆì¸¡ê³¼ ì •ë‹µ Tool Call ë¹„êµ (When-to-Call í¬í•¨)"""
    pred_has_tool_call = pred is not None
    
    result = {
        "gold_has_tool_call": gold_has_tool_call,
        "pred_has_tool_call": pred_has_tool_call,
        "when_to_call_correct": (pred_has_tool_call == gold_has_tool_call),
        "tool_selection_correct": False,
        "parameter_exact_match": False,
        "json_parse_success": False,
    }
    
    # Tool Callì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì„¸ë¶€ ë¹„êµ
    if gold_has_tool_call and gold:
        if pred:
            result["json_parse_success"] = not pred.get("parse_error", False)
            
            pred_name = pred.get("name", "").lower().strip()
            gold_name = gold.get("name", "").lower().strip()
            result["tool_selection_correct"] = (pred_name == gold_name)
            
            if result["tool_selection_correct"]:
                pred_args = pred.get("arguments", {})
                gold_args = gold.get("arguments", {})
                if isinstance(pred_args, dict) and isinstance(gold_args, dict):
                    result["parameter_exact_match"] = (pred_args == gold_args)
    
    return result


# ============================================================
# ë©”íŠ¸ë¦­ ê³„ì‚°
# ============================================================
def calculate_metrics(results: list[dict]) -> dict:
    """ì „ì²´ í‰ê°€ ê²°ê³¼ì—ì„œ ë©”íŠ¸ë¦­ ê³„ì‚°"""
    total = len(results)
    if total == 0:
        return {}
    
    # Tool Callì´ ìˆëŠ” ìƒ˜í”Œë§Œ í•„í„°ë§
    tool_call_samples = [r for r in results if r["gold_has_tool_call"]]
    no_call_samples = [r for r in results if not r["gold_has_tool_call"]]
    
    metrics = {
        "total_samples": total,
        "tool_call_samples": len(tool_call_samples),
        "no_call_samples": len(no_call_samples),
        
        # When-to-Call (ì „ì²´)
        "when_to_call_accuracy": sum(r["when_to_call_correct"] for r in results) / total,
        
        # Tool Call ìƒ˜í”Œì—ì„œì˜ ì •í™•ë„
        "tool_selection_accuracy": (
            sum(r["tool_selection_correct"] for r in tool_call_samples) / len(tool_call_samples)
            if tool_call_samples else 0
        ),
        "parameter_exact_match": (
            sum(r["parameter_exact_match"] for r in tool_call_samples) / len(tool_call_samples)
            if tool_call_samples else 0
        ),
        "json_parse_success_rate": (
            sum(r["json_parse_success"] for r in tool_call_samples) / len(tool_call_samples)
            if tool_call_samples else 0
        ),
        
        # No-Call ìƒ˜í”Œì—ì„œì˜ ì •í™•ë„ (False Positive Rate)
        "no_call_accuracy": (
            sum(r["when_to_call_correct"] for r in no_call_samples) / len(no_call_samples)
            if no_call_samples else 0
        ),
    }
    
    return metrics


# ============================================================
# í‰ê°€ ìƒ˜í”Œ ì¶”ì¶œ (ëª¨ë“  assistant í„´)
# ============================================================
def extract_all_eval_samples(messages: list, tools, source: str = "") -> list[dict]:
    """
    ëŒ€í™”ì—ì„œ ëª¨ë“  assistant í„´ì„ í‰ê°€ ìƒ˜í”Œë¡œ ì¶”ì¶œ (ë©€í‹°í„´ Tool Calling ì§€ì›)
    """
    if not messages:
        return []
    
    eval_samples = []
    
    for i, msg in enumerate(messages):
        if not isinstance(msg, dict) or msg.get("role") != "assistant":
            continue
        
        # ì´ assistant í„´ ì´ì „ê¹Œì§€ê°€ context
        context = messages[:i]
        
        # user ë©”ì‹œì§€ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ìŠ¤í‚µ
        if not any(m.get("role") == "user" for m in context if isinstance(m, dict)):
            continue
        
        # Tool Call ì¶”ì¶œ
        gold_tool_call = extract_tool_call_from_data(msg)
        has_tool_call = gold_tool_call is not None
        
        eval_samples.append({
            "context": context,
            "gold_response": msg.get("content", ""),
            "gold_tool_call": gold_tool_call,
            "has_tool_call": has_tool_call,
            "tools": tools,
            "source": source,
            "turn_index": i
        })
    
    return eval_samples


# ============================================================
# HuggingFace Hubì—ì„œ ë°ì´í„° ë¡œë“œ (ê· í˜• ìƒ˜í”Œë§)
# ============================================================
def load_test_data(dataset_id: str, num_samples: int, seed: int = 42):
    """HuggingFace Hubì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ (ëª¨ë“  í„´ ì¶”ì¶œ + ê· í˜• ìƒ˜í”Œë§)"""
    tool_call_samples = []
    no_call_samples = []
    
    print(f"\nğŸ“¥ HuggingFace Hubì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘: {dataset_id}")
    
    ds = load_dataset(dataset_id, split="train")
    print(f"âœ… {len(ds)}ê°œ ëŒ€í™” ë¡œë“œ")
    
    tc_count = 0
    nc_count = 0
    
    for sample in ds:
        # JSON ë¬¸ìì—´ íŒŒì‹± (í†µí•© ë°ì´í„°ì…‹ í˜•ì‹)
        messages = parse_messages(sample.get("messages", []))
        tools = parse_tools(sample.get("tools", []))
        
        # ëª¨ë“  assistant í„´ì„ í‰ê°€ ìƒ˜í”Œë¡œ ì¶”ì¶œ
        eval_samples = extract_all_eval_samples(messages, tools, dataset_id)
        for eval_sample in eval_samples:
            if eval_sample["has_tool_call"]:
                tool_call_samples.append(eval_sample)
                tc_count += 1
            else:
                no_call_samples.append(eval_sample)
                nc_count += 1
    
    print(f"   â†’ Tool Call í„´: {tc_count}ê°œ, No-Call í„´: {nc_count}ê°œ")
    print(f"\nğŸ“Š ì „ì²´ í‰ê°€ ê°€ëŠ¥ í„´: Tool Call {len(tool_call_samples)}ê°œ, No-Call {len(no_call_samples)}ê°œ")
    
    # ê· í˜• ìƒ˜í”Œë§
    random.seed(seed)
    
    half_samples = num_samples // 2
    
    random.shuffle(tool_call_samples)
    selected_tc = tool_call_samples[:min(half_samples, len(tool_call_samples))]
    
    random.shuffle(no_call_samples)
    selected_nc = no_call_samples[:min(half_samples, len(no_call_samples))]
    
    all_samples = selected_tc + selected_nc
    random.shuffle(all_samples)
    
    print(f"ğŸ“Š ìƒ˜í”Œë§ ê²°ê³¼: Tool Call {len(selected_tc)}ê°œ + No-Call {len(selected_nc)}ê°œ = {len(all_samples)}ê°œ")
    
    return all_samples


# ============================================================
# í”„ë¡¬í”„íŠ¸ ìƒì„±
# ============================================================
def parse_messages(messages) -> list:
    """JSON ë¬¸ìì—´ë¡œ ì €ì¥ëœ messagesë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ íŒŒì‹±"""
    if not messages:
        return []
    if isinstance(messages, str):
        try:
            return json.loads(messages)
        except json.JSONDecodeError:
            return []
    if isinstance(messages, list):
        return messages
    return []


def parse_tools(tools) -> list:
    """ë‹¤ì–‘í•œ í˜•ì‹ì˜ toolsë¥¼ í‘œì¤€ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    if not tools:
        return []
    
    if isinstance(tools, str):
        try:
            parsed = json.loads(tools)
            return parsed if isinstance(parsed, list) else [parsed]
        except json.JSONDecodeError:
            return []
    
    return tools if isinstance(tools, list) else []


def format_tools_for_prompt(tools: list) -> str:
    """toolsë¥¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ìš© ë¬¸ìì—´ë¡œ ë³€í™˜"""
    tools_descriptions = []
    
    for tool in tools:
        if isinstance(tool, str):
            try:
                tool = json.loads(tool)
            except json.JSONDecodeError:
                continue
        
        if not isinstance(tool, dict):
            continue
        
        name = tool.get("name", "")
        description = tool.get("description", "")
        params = tool.get("parameters", {})
        
        tool_json = json.dumps({
            "name": name,
            "description": description,
            "parameters": params,
            "required": tool.get("required", [])
        }, ensure_ascii=False)
        
        tools_descriptions.append(f"Use the function '{name}' to '{description}'\n{tool_json}")
    
    tools_text = "\n\n".join(tools_descriptions)
    
    return f"""You have access to the following functions:

{tools_text}

If you choose to call a function ONLY reply in the following format with no prefix or suffix:

<function=example_function_name>{{"example_name": "example_value"}}</function>

Reminder:
- Function calls MUST follow the specified format, start with <function= and end with </function>
- Required parameters MUST be specified
- Only call one function at a time
- Put the entire function call reply on one line
- If there is no function call available, answer the question like normal with your current knowledge and do not tell the user about function calls
"""


def create_eval_prompt(sample: dict, tokenizer) -> str:
    """í‰ê°€ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    tools = parse_tools(sample.get("tools", []))
    context = sample.get("context", [])
    
    system_content = format_tools_for_prompt(tools)
    messages = [{"role": "system", "content": system_content}]
    
    for msg in context:
        if not isinstance(msg, dict):
            continue
        role = msg.get("role", "")
        content = msg.get("content", "")
        
        if role == "tool":
            role = "ipython"
        
        # contentê°€ ì—†ëŠ” ê²½ìš° tool_callsì—ì„œ ìƒì„±
        if not content and role == "assistant":
            tool_calls = msg.get("tool_calls", [])
            if tool_calls and len(tool_calls) > 0:
                tc = tool_calls[0]
                func = tc.get("function", {})
                name = func.get("name", "")
                args = func.get("arguments", "{}")
                content = f"<function={name}>{args}</function>"
        
        if content:
            messages.append({"role": role, "content": content})
    
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    return prompt


# ============================================================
# í‰ê°€ ì‹¤í–‰ (ë°°ì¹˜ ì¶”ë¡ )
# ============================================================
def run_evaluation(
    model,
    tokenizer,
    test_samples: list,
    max_new_tokens: int = 256,
    batch_size: int = 8,
    progress_bar: bool = True
) -> list[dict]:
    """ë°°ì¹˜ í‰ê°€ ì‹¤í–‰"""
    results = []
    
    # íŒ¨ë”© ì„¤ì •
    tokenizer.padding_side = "left"
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
    num_batches = (len(test_samples) + batch_size - 1) // batch_size
    iterator = tqdm(range(num_batches), desc="í‰ê°€ ì¤‘") if progress_bar else range(num_batches)
    
    for batch_idx in iterator:
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, len(test_samples))
        batch_samples = test_samples[start_idx:end_idx]
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompts = [create_eval_prompt(s, tokenizer) for s in batch_samples]
        
        # ë°°ì¹˜ í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(
            prompts, 
            return_tensors="pt", 
            padding=True,
            truncation=True,
            max_length=8192
        ).to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        # ê²°ê³¼ ì²˜ë¦¬
        # ë°°ì¹˜ ë‚´ ëª¨ë“  ìƒ˜í”Œì€ ë™ì¼í•œ ê¸¸ì´ë¡œ íŒ¨ë”©ë¨
        input_tensor_len = inputs["input_ids"].shape[1]
        
        for i, sample in enumerate(batch_samples):
            # ì…ë ¥ í…ì„œ ê¸¸ì´ ì´í›„ê°€ ìƒì„±ëœ í† í°
            generated = outputs[i][input_tensor_len:]
            response = tokenizer.decode(generated, skip_special_tokens=True)
            
            # ì „ì²´ ì¶œë ¥ ì‹œí€€ìŠ¤ (ì…ë ¥ + ìƒì„±, íŠ¹ìˆ˜ í† í° í¬í•¨)
            full_output = tokenizer.decode(outputs[i], skip_special_tokens=False)
            
            pred_tool_call = extract_tool_call_from_response(response)
            gold_tool_call = sample.get("gold_tool_call")
            has_tool_call = sample.get("has_tool_call", False)
            
            comparison = compare_tool_calls(pred_tool_call, gold_tool_call, has_tool_call)
            
            result = {
                "source": sample.get("source", ""),
                "turn_index": sample.get("turn_index", 0),
                "has_tool_call": has_tool_call,
                "input_prompt": prompts[i],  # ì „ì²´ ì…ë ¥ í”„ë¡¬í”„íŠ¸
                "gold_response": sample.get("gold_response", ""),  # ì •ë‹µ ì‘ë‹µ
                "generated_output": response,  # ìƒˆë¡œ ìƒì„±ëœ í† í°ë§Œ
                "full_output_sequence": full_output,  # ì…ë ¥ + ìƒì„± ì „ì²´ (ë””ë²„ê¹…ìš©)
                "pred_tool_call": pred_tool_call,
                "gold_tool_call": gold_tool_call,
                **comparison
            }
            results.append(result)
    
    return results


# ============================================================
# ë©”ì¸ í•¨ìˆ˜
# ============================================================
def main():
    parser = argparse.ArgumentParser(description="Tool Calling ë² ì´ìŠ¤ ëª¨ë¸ í‰ê°€")
    
    parser.add_argument("--base_model", type=str, default="kakaocorp/kanana-nano-2.1b-instruct",
                        help="ë² ì´ìŠ¤ ëª¨ë¸ ID")
    parser.add_argument("--num_samples", type=int, default=100, help="í‰ê°€í•  ìƒ˜í”Œ ìˆ˜")
    parser.add_argument("--batch_size", type=int, default=1, help="ë°°ì¹˜ í¬ê¸° (unsloth í˜¸í™˜ì„± ë¬¸ì œë¡œ ê¸°ë³¸ê°’ 1)")
    parser.add_argument("--max_new_tokens", type=int, default=256, help="ìƒì„± ìµœëŒ€ í† í° ìˆ˜")
    parser.add_argument("--max_seq_length", type=int, default=16384, help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´")
    parser.add_argument("--output_dir", type=str, default="eval_results", help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--seed", type=int, default=42, help="ëœë¤ ì‹œë“œ")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ”¬ Tool Calling ë² ì´ìŠ¤ ëª¨ë¸ í‰ê°€")
    print("=" * 60)
    print(f"ë² ì´ìŠ¤ ëª¨ë¸: {args.base_model}")
    print(f"í‰ê°€ ìƒ˜í”Œ ìˆ˜: {args.num_samples}")
    print(f"ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    print(f"ìµœëŒ€ ìƒì„± í† í°: {args.max_new_tokens}")
    print("=" * 60)
    
    # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
    print("\nğŸš€ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=args.base_model,
        max_seq_length=args.max_seq_length,
        dtype=None,
        load_in_4bit=True,
    )
    
    # Unsloth ìµœì í™”ë¥¼ ìœ„í•´ dummy LoRA ì–´ëŒ‘í„° ì¶”ê°€
    model = FastLanguageModel.get_peft_model(
        model,
        r=8,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        lora_alpha=16,
        lora_dropout=0,
        bias="none",
        use_gradient_checkpointing="unsloth",
        random_state=42,
    )
    
    FastLanguageModel.for_inference(model)
    print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    test_samples = load_test_data(HF_DATASET, args.num_samples, args.seed)
    
    # í‰ê°€ ì‹¤í–‰ (ë°°ì¹˜ ì¶”ë¡ )
    print("\nğŸ”¬ í‰ê°€ ì‹œì‘...")
    results = run_evaluation(
        model, tokenizer, test_samples,
        max_new_tokens=args.max_new_tokens,
        batch_size=args.batch_size
    )
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    metrics = calculate_metrics(results)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“Š ë² ì´ìŠ¤ ëª¨ë¸ í‰ê°€ ê²°ê³¼")
    print("=" * 60)
    print(f"ì´ í‰ê°€ ìƒ˜í”Œ: {metrics.get('total_samples', 0)}")
    print(f"  - Tool Call ìƒ˜í”Œ: {metrics.get('tool_call_samples', 0)}")
    print(f"  - No-Call ìƒ˜í”Œ: {metrics.get('no_call_samples', 0)}")
    print("-" * 60)
    print(f"When-to-Call Accuracy: {metrics.get('when_to_call_accuracy', 0):.2%}")
    print(f"  - Tool Call ì •í™•ë„: {metrics.get('tool_selection_accuracy', 0):.2%}")
    print(f"  - No-Call ì •í™•ë„: {metrics.get('no_call_accuracy', 0):.2%}")
    print("-" * 60)
    print(f"Tool Selection Accuracy: {metrics.get('tool_selection_accuracy', 0):.2%}")
    print(f"Parameter Exact Match: {metrics.get('parameter_exact_match', 0):.2%}")
    print(f"JSON Parse Success Rate: {metrics.get('json_parse_success_rate', 0):.2%}")
    print("=" * 60)
    
    # ê²°ê³¼ ì €ì¥
    os.makedirs(args.output_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    results_df = pd.DataFrame(results)
    results_csv = os.path.join(args.output_dir, f"base_evaluation_results_{timestamp}.csv")
    results_df.to_csv(results_csv, index=False, encoding="utf-8-sig")
    print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {results_csv}")
    
    summary = {
        "model_type": "base",
        "base_model": args.base_model,
        "num_samples": args.num_samples,
        "batch_size": args.batch_size,
        "max_new_tokens": args.max_new_tokens,
        "timestamp": timestamp,
        "metrics": metrics
    }
    summary_json = os.path.join(args.output_dir, f"base_evaluation_summary_{timestamp}.json")
    with open(summary_json, "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ìš”ì•½ ì €ì¥: {summary_json}")
    
    print("\nâœ… ë² ì´ìŠ¤ ëª¨ë¸ í‰ê°€ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
