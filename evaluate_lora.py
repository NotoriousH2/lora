#!/usr/bin/env python3
"""
Tool Calling LoRA ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

í•™ìŠµëœ LoRA ëª¨ë¸ì˜ Tool Calling ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.

í‰ê°€ ì§€í‘œ:
- Tool Selection Accuracy: ì˜¬ë°”ë¥¸ Toolì„ ì„ íƒí–ˆëŠ”ì§€
- Parameter Exact Match: íŒŒë¼ë¯¸í„°ê°€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ”ì§€
- When-to-Call Accuracy: Tool í˜¸ì¶œ ì—¬ë¶€ íŒë‹¨ì´ ë§ëŠ”ì§€
- JSON Parse Success: ìƒì„±ëœ ì‘ë‹µì´ ìœ íš¨í•œ JSONì¸ì§€

ì‚¬ìš©ë²•:
    python evaluate_lora.py --model_path experiments/final_model
    python evaluate_lora.py --model_path experiments/final_model --num_samples 1000
    python evaluate_lora.py --model_path experiments/final_model --output_dir ./eval_results
"""

# ============================================================
# âš ï¸ UnslothëŠ” ë°˜ë“œì‹œ ë‹¤ë¥¸ íŒ¨í‚¤ì§€ë³´ë‹¤ ë¨¼ì € importí•´ì•¼ í•©ë‹ˆë‹¤!
# ============================================================
from unsloth import FastLanguageModel

import argparse
import json
import os
import re
from datetime import datetime
from typing import Optional

import pandas as pd
import torch
from datasets import load_dataset, concatenate_datasets
from tqdm import tqdm


# ============================================================
# HuggingFace Hub ë°ì´í„°ì…‹ ID
# ============================================================
HF_DATASETS = [
    "NotoriousH2/instructkr-toolflow",
    "NotoriousH2/instructkr-when2call",
    "NotoriousH2/instructkr-apigen",
]


# ============================================================
# Tool Call ì¶”ì¶œ í•¨ìˆ˜
# ============================================================
def extract_tool_call(response: str) -> Optional[dict]:
    """
    ëª¨ë¸ ì‘ë‹µì—ì„œ Tool Call ì¶”ì¶œ
    
    ì§€ì›í•˜ëŠ” í˜•ì‹:
    1. <function=name>{"param": "value"}</function>
    2. {"name": "...", "arguments": {...}}
    
    Returns:
        {"name": "tool_name", "arguments": {...}} ë˜ëŠ” None (Tool Call ì—†ìŒ)
    """
    if not response:
        return None
    
    # í˜•ì‹ 1: <function=name>{"param": "value"}</function>
    pattern1 = r'<function=([^>]+)>(.+?)</function>'
    match1 = re.search(pattern1, response, re.DOTALL)
    if match1:
        tool_name = match1.group(1).strip()
        try:
            arguments = json.loads(match1.group(2).strip())
            return {"name": tool_name, "arguments": arguments}
        except json.JSONDecodeError:
            return {"name": tool_name, "arguments": None, "parse_error": True}
    
    # í˜•ì‹ 2: {"name": "...", "arguments": {...}}
    try:
        # JSON ê°ì²´ ì°¾ê¸°
        json_pattern = r'\{[^{}]*"name"[^{}]*"arguments"[^{}]*\{[^{}]*\}[^{}]*\}'
        match2 = re.search(json_pattern, response, re.DOTALL)
        if match2:
            parsed = json.loads(match2.group())
            if "name" in parsed:
                return {
                    "name": parsed.get("name", ""),
                    "arguments": parsed.get("arguments", {})
                }
    except json.JSONDecodeError:
        pass
    
    # ê°„ë‹¨í•œ JSON í˜•ì‹: {"name": "...", "arguments": "..."}
    try:
        # ì „ì²´ ì‘ë‹µì´ JSONì¸ì§€ í™•ì¸
        parsed = json.loads(response.strip())
        if isinstance(parsed, dict) and "name" in parsed:
            return {
                "name": parsed.get("name", ""),
                "arguments": parsed.get("arguments", {})
            }
    except json.JSONDecodeError:
        pass
    
    return None


def extract_tool_call_from_gold(message: dict) -> Optional[dict]:
    """
    ì •ë‹µ ë©”ì‹œì§€ì—ì„œ Tool Call ì¶”ì¶œ
    
    assistant ë©”ì‹œì§€ì˜ contentê°€ Tool Call í˜•ì‹ì¸ ê²½ìš° ì¶”ì¶œ
    """
    content = message.get("content", "")
    return extract_tool_call(content)


# ============================================================
# ë¹„êµ í•¨ìˆ˜
# ============================================================
def compare_tool_calls(pred: Optional[dict], gold: Optional[dict]) -> dict:
    """
    ì˜ˆì¸¡ê³¼ ì •ë‹µ Tool Call ë¹„êµ
    
    Returns:
        {
            "tool_selection_correct": bool,
            "parameter_exact_match": bool,
            "when_to_call_correct": bool,
            "json_parse_success": bool,
        }
    """
    result = {
        "tool_selection_correct": False,
        "parameter_exact_match": False,
        "when_to_call_correct": False,
        "json_parse_success": False,
    }
    
    # When-to-Call: ë‘˜ ë‹¤ Noneì´ê±°ë‚˜ ë‘˜ ë‹¤ Tool Callì´ ìˆìœ¼ë©´ ì •ë‹µ
    pred_has_call = pred is not None
    gold_has_call = gold is not None
    result["when_to_call_correct"] = (pred_has_call == gold_has_call)
    
    # Tool Callì´ ì—†ëŠ” ê²½ìš°
    if not gold_has_call:
        if not pred_has_call:
            # ë‘˜ ë‹¤ Tool Call ì—†ìŒ - ì •ë‹µ
            result["tool_selection_correct"] = True
            result["parameter_exact_match"] = True
            result["json_parse_success"] = True
        return result
    
    if not pred_has_call:
        # ì •ë‹µì€ ìˆëŠ”ë° ì˜ˆì¸¡ì´ ì—†ìŒ
        return result
    
    # JSON íŒŒì‹± ì„±ê³µ ì—¬ë¶€
    result["json_parse_success"] = not pred.get("parse_error", False)
    
    # Tool ì´ë¦„ ë¹„êµ
    pred_name = pred.get("name", "").lower().strip()
    gold_name = gold.get("name", "").lower().strip()
    result["tool_selection_correct"] = (pred_name == gold_name)
    
    # íŒŒë¼ë¯¸í„° ë¹„êµ (Tool ì´ë¦„ì´ ë§ì„ ë•Œë§Œ)
    if result["tool_selection_correct"]:
        pred_args = pred.get("arguments", {})
        gold_args = gold.get("arguments", {})
        
        # ë‘˜ ë‹¤ dictì¸ ê²½ìš°ì—ë§Œ ë¹„êµ
        if isinstance(pred_args, dict) and isinstance(gold_args, dict):
            # í‚¤ì™€ ê°’ì´ ëª¨ë‘ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
            result["parameter_exact_match"] = (pred_args == gold_args)
        elif pred_args == gold_args:
            result["parameter_exact_match"] = True
    
    return result


# ============================================================
# ë©”íŠ¸ë¦­ ê³„ì‚°
# ============================================================
def calculate_metrics(results: list[dict]) -> dict:
    """ì „ì²´ í‰ê°€ ê²°ê³¼ì—ì„œ ë©”íŠ¸ë¦­ ê³„ì‚°"""
    total = len(results)
    if total == 0:
        return {}
    
    metrics = {
        "total_samples": total,
        "tool_selection_accuracy": sum(r["tool_selection_correct"] for r in results) / total,
        "parameter_exact_match": sum(r["parameter_exact_match"] for r in results) / total,
        "when_to_call_accuracy": sum(r["when_to_call_correct"] for r in results) / total,
        "json_parse_success_rate": sum(r["json_parse_success"] for r in results) / total,
    }
    
    # Tool Callì´ ìˆëŠ” ìƒ˜í”Œë§Œ ë”°ë¡œ ê³„ì‚°
    has_tool_call = [r for r in results if r.get("gold_has_tool_call", False)]
    if has_tool_call:
        metrics["tool_call_samples"] = len(has_tool_call)
        metrics["tool_selection_accuracy_on_calls"] = sum(
            r["tool_selection_correct"] for r in has_tool_call
        ) / len(has_tool_call)
        metrics["parameter_exact_match_on_calls"] = sum(
            r["parameter_exact_match"] for r in has_tool_call
        ) / len(has_tool_call)
    
    # Tool Callì´ ì—†ëŠ” ìƒ˜í”Œ
    no_tool_call = [r for r in results if not r.get("gold_has_tool_call", False)]
    if no_tool_call:
        metrics["no_tool_call_samples"] = len(no_tool_call)
        metrics["no_call_accuracy"] = sum(
            r["when_to_call_correct"] for r in no_tool_call
        ) / len(no_tool_call)
    
    return metrics


# ============================================================
# ë°ì´í„° ë¡œë“œ
# ============================================================
def load_test_data(dataset_ids: list[str], num_samples: int, seed: int = 42):
    """HuggingFace Hubì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
    all_datasets = []
    
    print("\nğŸ“¥ HuggingFace Hubì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
    
    for dataset_id in dataset_ids:
        try:
            ds = load_dataset(dataset_id, split="train")
            print(f"âœ… {dataset_id}: {len(ds)}ê°œ ìƒ˜í”Œ")
            all_datasets.append(ds)
        except Exception as e:
            print(f"âŒ {dataset_id} ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    if not all_datasets:
        raise ValueError("ë¡œë“œëœ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤!")
    
    # ë³‘í•©
    if len(all_datasets) == 1:
        combined = all_datasets[0]
    else:
        # ìŠ¤í‚¤ë§ˆ í†µì¼ì„ ìœ„í•´ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
        unified_datasets = []
        for ds in all_datasets:
            if "messages" in ds.column_names and "tools" in ds.column_names:
                unified_datasets.append(ds.select_columns(["messages", "tools"]))
        
        if unified_datasets:
            combined = concatenate_datasets(unified_datasets)
        else:
            combined = all_datasets[0]
    
    # ì…”í”Œ ë° ìƒ˜í”Œë§
    combined = combined.shuffle(seed=seed)
    
    if num_samples < len(combined):
        combined = combined.select(range(num_samples))
    
    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(combined)}ê°œ")
    
    return combined


# ============================================================
# í‰ê°€ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±
# ============================================================
def parse_tools(tools) -> list:
    """ë‹¤ì–‘í•œ í˜•ì‹ì˜ toolsë¥¼ í‘œì¤€ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    if not tools:
        return []
    
    if isinstance(tools, str):
        try:
            parsed = json.loads(tools)
            if isinstance(parsed, list):
                return parsed
            return [parsed]
        except json.JSONDecodeError:
            return []
    
    if isinstance(tools, list):
        return tools
    
    return []


def create_eval_prompt(messages: list, tools, tokenizer) -> tuple[str, Optional[dict]]:
    """
    í‰ê°€ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±
    
    Returns:
        (prompt, gold_tool_call)
        - prompt: ëª¨ë¸ì— ì…ë ¥í•  í”„ë¡¬í”„íŠ¸ (ë§ˆì§€ë§‰ assistant ì‘ë‹µ ì œì™¸)
        - gold_tool_call: ì •ë‹µ Tool Call (ìˆëŠ” ê²½ìš°)
    """
    # ë§ˆì§€ë§‰ assistant ì‘ë‹µ ì°¾ê¸°
    gold_response = None
    gold_tool_call = None
    
    # ë©”ì‹œì§€ì—ì„œ ë§ˆì§€ë§‰ assistant ì‘ë‹µ ë¶„ë¦¬
    prompt_messages = []
    for i, msg in enumerate(messages):
        if not isinstance(msg, dict):
            continue
        
        role = msg.get("role", "")
        
        # ë§ˆì§€ë§‰ assistant ì‘ë‹µ ì €ì¥
        if role == "assistant":
            gold_response = msg.get("content", "")
            gold_tool_call = extract_tool_call(gold_response)
            # ì´ì „ê¹Œì§€ì˜ ë©”ì‹œì§€ë§Œ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
            break
        
        prompt_messages.append(msg)
    
    # toolsë¥¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
    parsed_tools = parse_tools(tools)
    
    if parsed_tools:
        tools_text = format_tools_for_prompt(parsed_tools)
        system_msg = {"role": "system", "content": tools_text}
        prompt_messages = [system_msg] + prompt_messages
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    try:
        prompt = tokenizer.apply_chat_template(
            prompt_messages,
            tokenize=False,
            add_generation_prompt=True
        )
    except Exception:
        # Fallback
        prompt = "<|begin_of_text|>"
        for msg in prompt_messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            prompt += f"<|start_header_id|>{role}<|end_header_id|>\n\n{content}<|eot_id|>"
        prompt += "<|start_header_id|>assistant<|end_header_id|>\n\n"
    
    return prompt, gold_tool_call


def format_tools_for_prompt(tools: list) -> str:
    """toolsë¥¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ìš© ë¬¸ìì—´ë¡œ ë³€í™˜"""
    tools_descriptions = []
    
    for tool in tools:
        if isinstance(tool, str):
            try:
                tool = json.loads(tool)
            except json.JSONDecodeError:
                continue
        
        if not isinstance(tool, dict):
            continue
        
        name = tool.get("name", "")
        description = tool.get("description", "")
        params = tool.get("parameters", {})
        
        tool_json = json.dumps({
            "name": name,
            "description": description,
            "parameters": params,
            "required": tool.get("required", [])
        }, ensure_ascii=False)
        
        tools_descriptions.append(f"Use the function '{name}' to '{description}'\n{tool_json}")
    
    tools_text = "\n\n".join(tools_descriptions)
    
    return f"""You have access to the following functions:

{tools_text}

Think very carefully before calling functions.
If a you choose to call a function ONLY reply in the following format:
<{{start_tag}}={{function_name}}>{{parameters}}{{end_tag}}
where

start_tag => `<function`
parameters => a JSON dict with the function argument name as key and function argument value as value.
end_tag => `</function>`

Here is an example,
<function=example_function_name>{{"example_name": "example_value"}}</function>

Reminder:
- Function calls MUST follow the specified format, start with <function= and end with </function>
- Required parameters MUST be specified
- Only call one function at a time
- Put the entire function call reply on one line"""


# ============================================================
# ë©”ì¸ í‰ê°€ í•¨ìˆ˜
# ============================================================
def run_evaluation(model, tokenizer, test_data, max_new_tokens: int = 512) -> list[dict]:
    """ë©”ì¸ í‰ê°€ ë£¨í”„"""
    results = []
    
    # ì¶”ë¡  ëª¨ë“œë¡œ ì „í™˜
    FastLanguageModel.for_inference(model)
    
    for idx, sample in enumerate(tqdm(test_data, desc="í‰ê°€ ì§„í–‰")):
        messages = sample.get("messages", [])
        tools = sample.get("tools", [])
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        try:
            prompt, gold_tool_call = create_eval_prompt(messages, tools, tokenizer)
        except Exception as e:
            print(f"âš ï¸ ìƒ˜í”Œ {idx} í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            continue
        
        # ëª¨ë¸ ì¶”ë¡ 
        try:
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=False,
                    temperature=None,
                    top_p=None,
                    pad_token_id=tokenizer.eos_token_id,
                )
            
            # ì‘ë‹µ ë””ì½”ë”©
            response = tokenizer.decode(
                outputs[0][inputs["input_ids"].shape[1]:],
                skip_special_tokens=True
            ).strip()
            
        except Exception as e:
            print(f"âš ï¸ ìƒ˜í”Œ {idx} ì¶”ë¡  ì‹¤íŒ¨: {e}")
            response = ""
        
        # Tool Call ì¶”ì¶œ
        pred_tool_call = extract_tool_call(response)
        
        # ë¹„êµ
        comparison = compare_tool_calls(pred_tool_call, gold_tool_call)
        
        # ê²°ê³¼ ì €ì¥
        result = {
            "sample_idx": idx,
            "gold_has_tool_call": gold_tool_call is not None,
            "pred_has_tool_call": pred_tool_call is not None,
            "gold_tool_name": gold_tool_call.get("name", "") if gold_tool_call else "",
            "pred_tool_name": pred_tool_call.get("name", "") if pred_tool_call else "",
            "response_preview": response[:200] if response else "",
            **comparison
        }
        results.append(result)
    
    return results


# ============================================================
# ê²°ê³¼ ì €ì¥
# ============================================================
def save_results(results: list[dict], metrics: dict, output_dir: str):
    """í‰ê°€ ê²°ê³¼ ì €ì¥"""
    os.makedirs(output_dir, exist_ok=True)
    
    # CSV ì €ì¥
    csv_path = os.path.join(output_dir, "evaluation_results.csv")
    df = pd.DataFrame(results)
    df.to_csv(csv_path, index=False, encoding="utf-8")
    print(f"âœ… ìƒì„¸ ê²°ê³¼ ì €ì¥: {csv_path}")
    
    # JSON ìš”ì•½ ì €ì¥
    summary_path = os.path.join(output_dir, "evaluation_summary.json")
    summary = {
        "timestamp": datetime.now().isoformat(),
        "metrics": metrics,
    }
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    print(f"âœ… ìš”ì•½ ì €ì¥: {summary_path}")


def print_metrics(metrics: dict):
    """ë©”íŠ¸ë¦­ ì¶œë ¥"""
    print("\n" + "=" * 60)
    print("ğŸ“Š í‰ê°€ ê²°ê³¼")
    print("=" * 60)
    print(f"ì´ ìƒ˜í”Œ ìˆ˜: {metrics.get('total_samples', 0)}")
    print()
    print("ì „ì²´ ì •í™•ë„:")
    print(f"  - Tool Selection Accuracy: {metrics.get('tool_selection_accuracy', 0):.2%}")
    print(f"  - Parameter Exact Match:   {metrics.get('parameter_exact_match', 0):.2%}")
    print(f"  - When-to-Call Accuracy:   {metrics.get('when_to_call_accuracy', 0):.2%}")
    print(f"  - JSON Parse Success:      {metrics.get('json_parse_success_rate', 0):.2%}")
    
    if "tool_call_samples" in metrics:
        print()
        print(f"Tool Callì´ í•„ìš”í•œ ìƒ˜í”Œ ({metrics['tool_call_samples']}ê°œ):")
        print(f"  - Tool Selection: {metrics.get('tool_selection_accuracy_on_calls', 0):.2%}")
        print(f"  - Parameter Match: {metrics.get('parameter_exact_match_on_calls', 0):.2%}")
    
    if "no_tool_call_samples" in metrics:
        print()
        print(f"Tool Callì´ ë¶ˆí•„ìš”í•œ ìƒ˜í”Œ ({metrics['no_tool_call_samples']}ê°œ):")
        print(f"  - No-Call Accuracy: {metrics.get('no_call_accuracy', 0):.2%}")
    
    print("=" * 60)


# ============================================================
# ë©”ì¸
# ============================================================
def main():
    parser = argparse.ArgumentParser(
        description="Tool Calling LoRA ëª¨ë¸ í‰ê°€",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument(
        "--model_path", type=str, required=True,
        help="í‰ê°€í•  ëª¨ë¸ ê²½ë¡œ (LoRA ì–´ëŒ‘í„° ë˜ëŠ” ì „ì²´ ëª¨ë¸)"
    )
    parser.add_argument(
        "--num_samples", type=int, default=500,
        help="í‰ê°€í•  ìƒ˜í”Œ ìˆ˜"
    )
    parser.add_argument(
        "--output_dir", type=str, default="./eval_results",
        help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬"
    )
    parser.add_argument(
        "--max_new_tokens", type=int, default=512,
        help="ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜"
    )
    parser.add_argument(
        "--max_seq_length", type=int, default=4096,
        help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´"
    )
    parser.add_argument(
        "--seed", type=int, default=42,
        help="Random seed"
    )
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ” Tool Calling LoRA ëª¨ë¸ í‰ê°€")
    print("=" * 60)
    print(f"ëª¨ë¸ ê²½ë¡œ: {args.model_path}")
    print(f"ìƒ˜í”Œ ìˆ˜: {args.num_samples}")
    print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output_dir}")
    print("=" * 60)
    
    # ëª¨ë¸ ë¡œë“œ
    print("\nğŸ“¦ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=args.model_path,
        max_seq_length=args.max_seq_length,
        dtype=None,
        load_in_4bit=True,
    )
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    test_data = load_test_data(HF_DATASETS, args.num_samples, args.seed)
    
    # í‰ê°€ ì‹¤í–‰
    print("\nğŸƒ í‰ê°€ ì‹œì‘...")
    results = run_evaluation(model, tokenizer, test_data, args.max_new_tokens)
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    metrics = calculate_metrics(results)
    
    # ê²°ê³¼ ì¶œë ¥
    print_metrics(metrics)
    
    # ê²°ê³¼ ì €ì¥
    save_results(results, metrics, args.output_dir)
    
    print("\nğŸ‰ í‰ê°€ ì™„ë£Œ!")


if __name__ == "__main__":
    main()

