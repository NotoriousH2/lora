#!/usr/bin/env python3
"""
Kanana Tool Calling LoRA Training Script
ë‹¤ì–‘í•œ LoRA Rankì™€ ëª¨ë¸ì— ëŒ€í•œ ì‹¤í—˜ì„ ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python train_lora.py --model kakaocorp/kanana-1.5-2.1b-instruct-2505 --rank 16
    python train_lora.py --model kakaocorp/kanana-1.5-2.1b-instruct-2505 --rank 32 --lr 1e-4
    
ì˜ˆì‹œ (ë‹¤ì–‘í•œ rank ì‹¤í—˜):
    python train_lora.py --rank 4
    python train_lora.py --rank 8
    python train_lora.py --rank 16
    python train_lora.py --rank 32
    python train_lora.py --rank 64

ë°ì´í„°ì…‹:
    ê¸°ë³¸ì ìœ¼ë¡œ HuggingFace Hubì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤:
    - NotoriousH2/instructkr-toolflow
    - NotoriousH2/instructkr-when2call
    - NotoriousH2/instructkr-apigen
    
    ë¡œì»¬ íŒŒì¼ì„ ì‚¬ìš©í•˜ë ¤ë©´ --local_data í”Œë˜ê·¸ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.
"""

# ============================================================
# âš ï¸ UnslothëŠ” ë°˜ë“œì‹œ ë‹¤ë¥¸ íŒ¨í‚¤ì§€ë³´ë‹¤ ë¨¼ì € importí•´ì•¼ í•©ë‹ˆë‹¤!
# (Unslothê°€ transformers, torch ë“±ì„ monkey-patchí•˜ê¸° ë•Œë¬¸)
# ============================================================
from unsloth import FastLanguageModel

import argparse
import json
import os
import random
from datetime import datetime
from functools import partial

import numpy as np
import torch
from datasets import Dataset, load_dataset, concatenate_datasets
from transformers import TrainerCallback
from trl import SFTTrainer, SFTConfig


# ============================================================
# HuggingFace Hub ë°ì´í„°ì…‹ ID
# ============================================================
HF_DATASETS = [
    "NotoriousH2/instructkr-toolflow",
    "NotoriousH2/instructkr-when2call", 
    "NotoriousH2/instructkr-apigen",
]


# ============================================================
# Tool Calling ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
# ============================================================
TOOL_SYSTEM_PROMPT_TEMPLATE = """You have access to the following functions:

{tools_description}

Think very carefully before calling functions.
If a you choose to call a function ONLY reply in the following format:
<{{start_tag}}={{function_name}}>{{parameters}}{{end_tag}}
where

start_tag => `<function`
parameters => a JSON dict with the function argument name as key and function argument value as value.
end_tag => `</function>`

Here is an example,
<function=example_function_name>{{"example_name": "example_value"}}</function>

Reminder:
- If looking for real time information use relevant functions before falling back to brave_search
- Function calls MUST follow the specified format, start with <function= and end with </function>
- Required parameters MUST be specified
- Only call one function at a time
- Put the entire function call reply on one line"""


# ============================================================
# messages + tools â†’ text ë³€í™˜ í•¨ìˆ˜
# ============================================================
def format_tools_for_system_prompt(tools: list) -> str:
    """tools ë¦¬ìŠ¤íŠ¸ë¥¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ìš© ë¬¸ìì—´ë¡œ ë³€í™˜"""
    if not tools:
        return ""
    
    tools_descriptions = []
    for tool in tools:
        # toolì´ ë¬¸ìì—´ì¸ ê²½ìš° JSON íŒŒì‹±
        if isinstance(tool, str):
            try:
                tool_dict = json.loads(tool)
            except json.JSONDecodeError:
                continue
        else:
            tool_dict = tool
        
        name = tool_dict.get("name", "")
        description = tool_dict.get("description", "")
        
        # parametersë¥¼ JSON Schema í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        params = tool_dict.get("parameters", {})
        if "properties" in params:
            # typeì„ dict â†’ objectë¡œ ë³€í™˜
            if params.get("type") == "dict":
                params["type"] = "object"
            # properties ë‚´ type ë³€í™˜
            for prop_name, prop_value in params.get("properties", {}).items():
                if prop_value.get("type") == "str":
                    prop_value["type"] = "string"
                elif prop_value.get("type") == "int":
                    prop_value["type"] = "integer"
        
        tool_json = json.dumps({
            "name": name,
            "description": description,
            "parameters": params,
            "required": tool_dict.get("required", [])
        }, ensure_ascii=False)
        
        tools_descriptions.append(f"Use the function '{name}' to '{description}'\n{tool_json}")
    
    return "\n\n".join(tools_descriptions)


def parse_tools(tools) -> list:
    """
    ë‹¤ì–‘í•œ í˜•ì‹ì˜ toolsë¥¼ í‘œì¤€ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    
    ì§€ì› í˜•ì‹:
    - ["{json1}", "{json2}"] - ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
    - "[{json1}, {json2}]" - ì „ì²´ê°€ í•˜ë‚˜ì˜ JSON ë¬¸ìì—´
    - [{"name": ...}, ...] - ì´ë¯¸ íŒŒì‹±ëœ dict ë¦¬ìŠ¤íŠ¸
    - None ë˜ëŠ” ë¹ˆ ê°’
    """
    if not tools:
        return []
    
    # ë¬¸ìì—´ì¸ ê²½ìš° (ì „ì²´ ë°°ì—´ì´ JSON ë¬¸ìì—´ë¡œ ì¸ì½”ë”©ëœ ê²½ìš°)
    if isinstance(tools, str):
        try:
            parsed = json.loads(tools)
            if isinstance(parsed, list):
                return parsed
            return [parsed]
        except json.JSONDecodeError:
            return []
    
    # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
    if isinstance(tools, list):
        return tools
    
    return []


def convert_messages_tools_to_text(messages: list, tools, tokenizer) -> str:
    """
    messages + tools í˜•ì‹ì„ Llama 3 chat template textë¡œ ë³€í™˜
    
    Args:
        messages: [{"role": "user/assistant/system", "content": "..."}]
        tools: ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì› (ë¦¬ìŠ¤íŠ¸, JSON ë¬¸ìì—´ ë“±)
        tokenizer: í† í¬ë‚˜ì´ì € (chat_template ì ìš©ìš©)
    
    Returns:
        Llama 3 í¬ë§·ì˜ text ë¬¸ìì—´
    """
    # toolsë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ íŒŒì‹±
    parsed_tools = parse_tools(tools)
    
    # toolsê°€ ìˆìœ¼ë©´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    formatted_messages = []
    
    if parsed_tools:
        tools_description = format_tools_for_system_prompt(parsed_tools)
        if tools_description:
            system_content = TOOL_SYSTEM_PROMPT_TEMPLATE.format(tools_description=tools_description)
            formatted_messages.append({"role": "system", "content": system_content})
    
    # ê¸°ì¡´ messages ì¶”ê°€ (ì´ë¯¸ systemì´ ìˆìœ¼ë©´ ë³‘í•© ê³ ë ¤)
    for msg in messages:
        # msgê°€ dictê°€ ì•„ë‹ˆë©´ ìŠ¤í‚µ
        if not isinstance(msg, dict):
            continue
            
        role = msg.get("role", "")
        content = msg.get("content", "")
        
        # roleì´ ì—†ê±°ë‚˜ contentê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
        if not role:
            continue
        
        # tool roleì€ ipythonìœ¼ë¡œ ë³€í™˜ (Llama 3 í˜•ì‹)
        if role == "tool":
            role = "ipython"
        
        # ê¸°ì¡´ system messageê°€ ìˆìœ¼ë©´ tools systemê³¼ ë³‘í•©
        if role == "system" and formatted_messages and formatted_messages[0]["role"] == "system":
            formatted_messages[0]["content"] = formatted_messages[0]["content"] + "\n\n" + content
        else:
            formatted_messages.append({"role": role, "content": content})
    
    # chat template ì ìš©
    try:
        text = tokenizer.apply_chat_template(
            formatted_messages,
            tokenize=False,
            add_generation_prompt=False
        )
    except Exception as e:
        # fallback: ìˆ˜ë™ìœ¼ë¡œ Llama 3 í˜•ì‹ ìƒì„±
        text = "<|begin_of_text|>"
        for msg in formatted_messages:
            role = msg["role"]
            content = msg["content"]
            text += f"<|start_header_id|>{role}<|end_header_id|>\n\n{content}<|eot_id|>"
    
    return text


def convert_dataset_to_text(dataset, tokenizer):
    """
    Datasetì˜ messages + toolsë¥¼ textë¡œ ë³€í™˜
    
    ì§€ì›í•˜ëŠ” í˜•ì‹:
    - {"text": "..."} - ì´ë¯¸ ë³€í™˜ëœ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
    - {"messages": [...], "tools": [...]} - ë³€í™˜ í•„ìš”
    """
    def convert_example(example):
        # ì´ë¯¸ text í•„ë“œê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if "text" in example and example["text"]:
            return example
        
        messages = example.get("messages", [])
        tools = example.get("tools", [])
        
        text = convert_messages_tools_to_text(messages, tools, tokenizer)
        return {"text": text}
    
    # ì²« ë²ˆì§¸ ìƒ˜í”Œë¡œ í˜•ì‹ í™•ì¸
    first_example = dataset[0]
    if "text" in first_example and first_example["text"]:
        print("âœ… ë°ì´í„°ì…‹ì— ì´ë¯¸ 'text' í•„ë“œê°€ ìˆìŠµë‹ˆë‹¤.")
        return dataset
    
    print("ğŸ”„ messages + tools â†’ text ë³€í™˜ ì¤‘...")
    converted_dataset = dataset.map(
        convert_example,
        desc="ë°ì´í„° ë³€í™˜"
    )
    print("âœ… ë³€í™˜ ì™„ë£Œ")
    
    return converted_dataset


# ============================================================
# CSV ë¡œê¹… ì½œë°±
# ============================================================
class CSVLoggingCallback(TrainerCallback):
    """í•™ìŠµ ë¡œê·¸ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ì½œë°±"""
    
    def __init__(self, csv_path: str, experiment_info: dict):
        self.csv_path = csv_path
        self.experiment_info = experiment_info
        self.logs = []
        
        # CSV í—¤ë” ì‘ì„±
        with open(self.csv_path, 'w', encoding='utf-8') as f:
            # ë©”íƒ€ ì •ë³´ë¥¼ ì£¼ì„ìœ¼ë¡œ ì €ì¥
            f.write(f"# experiment_info: {json.dumps(experiment_info, ensure_ascii=False)}\n")
            f.write("step,epoch,train_loss,eval_loss,learning_rate,timestamp\n")
    
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is None:
            return
        
        row = {
            'step': state.global_step,
            'epoch': round(state.epoch, 4) if state.epoch else 0,
            'train_loss': logs.get('loss', ''),
            'eval_loss': logs.get('eval_loss', ''),
            'learning_rate': logs.get('learning_rate', ''),
            'timestamp': datetime.now().isoformat()
        }
        
        with open(self.csv_path, 'a', encoding='utf-8') as f:
            f.write(f"{row['step']},{row['epoch']},{row['train_loss']},{row['eval_loss']},{row['learning_rate']},{row['timestamp']}\n")


# ============================================================
# Labels ìƒì„± í•¨ìˆ˜ (ìƒŒë“œìœ„ì¹˜ ë§ˆìŠ¤í‚¹)
# ============================================================
def create_labels_for_tool_calling(text: str, tokenizer, max_length: int = 9000) -> dict:
    """
    Tool Calling í•™ìŠµì„ ìœ„í•œ labels ìƒì„± í•¨ìˆ˜ (ìƒŒë“œìœ„ì¹˜ ë§ˆìŠ¤í‚¹)
    
    - system, user, tool/ipython ë¸”ë¡: ë§ˆìŠ¤í‚¹ (-100)
    - assistant ë¸”ë¡: í•™ìŠµ (ì‹¤ì œ token_id)
    """
    encoding = tokenizer(text, truncation=True, max_length=max_length, return_tensors=None)
    input_ids = encoding['input_ids']
    attention_mask = encoding['attention_mask']
    
    labels = [-100] * len(input_ids)
    
    assistant_start = "<|start_header_id|>assistant<|end_header_id|>"
    tool_start = "<|start_header_id|>ipython<|end_header_id|>"
    eot_token = "<|eot_id|>"
    
    assistant_token_ids = tokenizer.encode(assistant_start, add_special_tokens=False)
    tool_token_ids = tokenizer.encode(tool_start, add_special_tokens=False)
    eot_token_ids = tokenizer.encode(eot_token, add_special_tokens=False)
    
    def find_all_positions(sequence, pattern):
        positions = []
        if len(pattern) == 0:
            return positions
        for i in range(len(sequence) - len(pattern) + 1):
            if sequence[i:i+len(pattern)] == pattern:
                positions.append(i)
        return positions
    
    assistant_positions = find_all_positions(input_ids, assistant_token_ids)
    tool_positions = find_all_positions(input_ids, tool_token_ids)
    eot_positions = find_all_positions(input_ids, eot_token_ids)
    
    for asst_pos in assistant_positions:
        start_pos = asst_pos + len(assistant_token_ids)
        
        end_pos = None
        for eot_pos in eot_positions:
            if eot_pos > asst_pos:
                end_pos = eot_pos + len(eot_token_ids)
                break
        
        if end_pos is None:
            end_pos = len(input_ids)
        
        is_followed_by_tool = any(asst_pos < tp < end_pos for tp in tool_positions)
        
        if not is_followed_by_tool or asst_pos < min(tool_positions, default=float('inf')):
            for i in range(start_pos, end_pos):
                if i < len(labels):
                    labels[i] = input_ids[i]
    
    for tool_pos in tool_positions:
        start_pos = tool_pos
        end_pos = None
        for eot_pos in eot_positions:
            if eot_pos > tool_pos:
                end_pos = eot_pos + len(eot_token_ids)
                break
        
        if end_pos is None:
            end_pos = len(input_ids)
        
        for i in range(start_pos, end_pos):
            if i < len(labels):
                labels[i] = -100
    
    return {
        'input_ids': input_ids,
        'attention_mask': attention_mask,
        'labels': labels
    }


def preprocess_function(examples, tokenizer, max_seq_length=9000):
    """ë°°ì¹˜ ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    all_input_ids = []
    all_attention_masks = []
    all_labels = []
    
    texts = examples['text']
    if isinstance(texts, str):
        texts = [texts]
    
    for text in texts:
        result = create_labels_for_tool_calling(text, tokenizer, max_seq_length)
        all_input_ids.append(result['input_ids'])
        all_attention_masks.append(result['attention_mask'])
        all_labels.append(result['labels'])
    
    return {
        'input_ids': all_input_ids,
        'attention_mask': all_attention_masks,
        'labels': all_labels
    }


# ============================================================
# ë°ì´í„° ë¡œë“œ
# ============================================================
def load_training_data_from_hub(dataset_ids: list[str], tokenizer, seed: int = 42):
    """HuggingFace Hubì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ ë° ë³‘í•©"""
    all_datasets = []
    
    print("\nğŸ“¥ HuggingFace Hubì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    
    for dataset_id in dataset_ids:
        try:
            ds = load_dataset(dataset_id, split="train")
            print(f"âœ… {dataset_id}: {len(ds)}ê°œ ìƒ˜í”Œ ë¡œë“œ")
            
            # ê° ë°ì´í„°ì…‹ì„ ë¨¼ì € textë¡œ ë³€í™˜ (ìŠ¤í‚¤ë§ˆ í†µì¼)
            ds = convert_dataset_to_text(ds, tokenizer)
            
            # text í•„ë“œë§Œ ìœ ì§€ (ìŠ¤í‚¤ë§ˆ ì°¨ì´ ë¬¸ì œ í•´ê²°)
            if "text" in ds.column_names:
                ds = ds.select_columns(["text"])
            
            all_datasets.append(ds)
        except Exception as e:
            print(f"âŒ {dataset_id} ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    if not all_datasets:
        raise ValueError("ë¡œë“œëœ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤!")
    
    # ë°ì´í„°ì…‹ ë³‘í•© (ëª¨ë‘ {"text": ...} í˜•ì‹ìœ¼ë¡œ í†µì¼ë¨)
    if len(all_datasets) == 1:
        combined_dataset = all_datasets[0]
    else:
        combined_dataset = concatenate_datasets(all_datasets)
    
    print(f"\nğŸ“Š ì´ ë°ì´í„° ìˆ˜: {len(combined_dataset)}ê°œ")
    
    # ì…”í”Œ ë° ë¶„í• 
    combined_dataset = combined_dataset.shuffle(seed=seed)
    
    split = combined_dataset.train_test_split(test_size=0.1, seed=seed)
    train_dataset = split['train']
    valid_dataset = split['test']
    
    print(f"Train ë°ì´í„°: {len(train_dataset)}ê°œ")
    print(f"Valid ë°ì´í„°: {len(valid_dataset)}ê°œ")
    
    return train_dataset, valid_dataset


def load_training_data_from_local(data_files: list[str], seed: int = 42):
    """ë¡œì»¬ íŒŒì¼ì—ì„œ í•™ìŠµ ë°ì´í„° ë¡œë“œ ë° ë¶„í• """
    all_data = []
    
    print("\nğŸ“‚ ë¡œì»¬ íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ ì¤‘...")
    
    for file_path in data_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"âœ… {file_path}: {len(data)}ê°œ ìƒ˜í”Œ ë¡œë“œ")
            all_data.extend(data)
        except FileNotFoundError:
            print(f"âš ï¸ {file_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ {file_path} ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    if not all_data:
        raise ValueError("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
    
    print(f"\nğŸ“Š ì´ ë°ì´í„° ìˆ˜: {len(all_data)}ê°œ")
    
    random.seed(seed)
    random.shuffle(all_data)
    
    split_idx = int(len(all_data) * 0.9)
    train_data = all_data[:split_idx]
    valid_data = all_data[split_idx:]
    
    print(f"Train ë°ì´í„°: {len(train_data)}ê°œ")
    print(f"Valid ë°ì´í„°: {len(valid_data)}ê°œ")
    
    # Dataset ê°ì²´ë¡œ ë³€í™˜
    train_dataset = Dataset.from_list(train_data)
    valid_dataset = Dataset.from_list(valid_data)
    
    return train_dataset, valid_dataset


# ============================================================
# ì‹¤í—˜ ì´ë¦„ ìƒì„±
# ============================================================
def generate_experiment_name(model_name: str, rank: int, lr: float, epochs: int) -> str:
    """ì‹¤í—˜ ì‹ë³„ì„ ìœ„í•œ ê³ ìœ  ì´ë¦„ ìƒì„±"""
    # ëª¨ë¸ ì´ë¦„ì—ì„œ í•µì‹¬ ë¶€ë¶„ ì¶”ì¶œ
    model_short = model_name.split('/')[-1].replace('-', '_')
    
    # learning rate í¬ë§·íŒ…
    lr_str = f"{lr:.0e}".replace('-', 'm').replace('+', 'p')
    
    # íƒ€ì„ìŠ¤íƒ¬í”„
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    return f"{model_short}_r{rank}_a{rank*2}_lr{lr_str}_ep{epochs}_{timestamp}"


# ============================================================
# ë©”ì¸ í•™ìŠµ í•¨ìˆ˜
# ============================================================
def train(args):
    """ë©”ì¸ í•™ìŠµ í•¨ìˆ˜"""
    
    # ì‹¤í—˜ ì´ë¦„ ìƒì„±
    experiment_name = generate_experiment_name(
        args.model, args.rank, args.lr, args.epochs
    )
    
    print("=" * 70)
    print(f"ğŸš€ ì‹¤í—˜ ì‹œì‘: {experiment_name}")
    print("=" * 70)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    output_dir = os.path.join(args.output_dir, experiment_name)
    os.makedirs(output_dir, exist_ok=True)
    
    # CSV ë¡œê·¸ ê²½ë¡œ
    csv_log_path = os.path.join(output_dir, f"training_log.csv")
    
    # ì‹¤í—˜ ì •ë³´
    experiment_info = {
        'model': args.model,
        'rank': args.rank,
        'alpha': args.rank * 2,
        'learning_rate': args.lr,
        'epochs': args.epochs,
        'batch_size': args.batch_size,
        'gradient_accumulation': args.grad_accum,
        'max_seq_length': args.max_seq_length,
        'seed': args.seed,
        'experiment_name': experiment_name,
        'data_source': 'local' if args.local_data else 'huggingface_hub',
        'datasets': args.data_files if args.local_data else args.hf_datasets,
        'start_time': datetime.now().isoformat()
    }
    
    # ì‹¤í—˜ ì„¤ì • ì €ì¥
    config_path = os.path.join(output_dir, "experiment_config.json")
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(experiment_info, f, indent=2, ensure_ascii=False)
    print(f"ğŸ“ ì‹¤í—˜ ì„¤ì • ì €ì¥: {config_path}")
    
    # ============================================================
    # ëª¨ë¸ ë¡œë“œ
    # ============================================================
    print(f"\nğŸ“¦ ëª¨ë¸ ë¡œë“œ ì¤‘: {args.model}")
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=args.model,
        max_seq_length=args.max_seq_length,
        dtype=None,
        load_in_4bit=True,
    )
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    # ============================================================
    # LoRA ì„¤ì •
    # ============================================================
    print(f"\nğŸ”§ LoRA ì„¤ì •: rank={args.rank}, alpha={args.rank * 2}")
    model = FastLanguageModel.get_peft_model(
        model,
        r=args.rank,
        lora_alpha=args.rank * 2,  # Alpha = Rank * 2
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",
            "up_proj", "down_proj", "gate_proj"
        ],
        lora_dropout=0,
        bias="none",
        use_gradient_checkpointing="unsloth",
        random_state=args.seed,
    )
    model.print_trainable_parameters()
    
    # ============================================================
    # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    # ============================================================
    if args.local_data:
        # ë¡œì»¬ íŒŒì¼ì—ì„œ ë¡œë“œ (ì´ë¯¸ text í•„ë“œê°€ ìˆëŠ” ê²½ìš°)
        raw_train_dataset, raw_valid_dataset = load_training_data_from_local(
            args.data_files, args.seed
        )
    else:
        # HuggingFace Hubì—ì„œ ë¡œë“œ (messages + tools â†’ text ë³€í™˜ í¬í•¨)
        raw_train_dataset, raw_valid_dataset = load_training_data_from_hub(
            args.hf_datasets, tokenizer, args.seed
        )
    
    preprocess_fn = partial(
        preprocess_function, 
        tokenizer=tokenizer, 
        max_seq_length=args.max_seq_length
    )
    
    train_dataset = raw_train_dataset.map(
        preprocess_fn,
        batched=True,
        remove_columns=raw_train_dataset.column_names,
        desc="Train ë°ì´í„° ì „ì²˜ë¦¬"
    )
    
    valid_dataset = raw_valid_dataset.map(
        preprocess_fn,
        batched=True,
        remove_columns=raw_valid_dataset.column_names,
        desc="Valid ë°ì´í„° ì „ì²˜ë¦¬"
    )
    
    print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: Train {len(train_dataset)}, Valid {len(valid_dataset)}")
    
    # ============================================================
    # SFTTrainer ì„¤ì •
    # ============================================================
    sft_config = SFTConfig(
        output_dir=output_dir,
        max_seq_length=args.max_seq_length,
        
        # í•™ìŠµ ì„¤ì •
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        gradient_accumulation_steps=args.grad_accum,
        
        # ì˜µí‹°ë§ˆì´ì €
        learning_rate=args.lr,
        weight_decay=args.weight_decay,
        warmup_ratio=args.warmup_ratio,
        optim="adamw_8bit",
        
        # ë¡œê¹… ë° ì €ì¥
        eval_strategy="steps",
        eval_steps=args.eval_steps,
        save_strategy="steps",
        save_steps=args.save_steps,
        save_total_limit=3,
        logging_steps=args.logging_steps,
        
        # í‰ê°€
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        
        # ê¸°íƒ€
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        seed=args.seed,
        report_to="none",
    )
    
    # CSV ë¡œê¹… ì½œë°±
    csv_callback = CSVLoggingCallback(csv_log_path, experiment_info)
    
    # Trainer ìƒì„±
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        eval_dataset=valid_dataset,
        args=sft_config,
        callbacks=[csv_callback],
    )
    
    # ============================================================
    # í•™ìŠµ ì‹¤í–‰
    # ============================================================
    print("\n" + "=" * 70)
    print("ğŸƒ í•™ìŠµ ì‹œì‘")
    print("=" * 70)
    print(f"ëª¨ë¸: {args.model}")
    print(f"LoRA Rank: {args.rank}, Alpha: {args.rank * 2}")
    print(f"Learning Rate: {args.lr}")
    print(f"Batch Size: {args.batch_size} Ã— {args.grad_accum} = {args.batch_size * args.grad_accum}")
    print(f"Epochs: {args.epochs}")
    print(f"Max Seq Length: {args.max_seq_length}")
    print(f"Logging Steps: {args.logging_steps}")
    print(f"Eval Steps: {args.eval_steps}")
    print(f"ë°ì´í„° ì†ŒìŠ¤: {'ë¡œì»¬ íŒŒì¼' if args.local_data else 'HuggingFace Hub'}")
    print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print(f"CSV ë¡œê·¸: {csv_log_path}")
    print("=" * 70 + "\n")
    
    trainer_stats = trainer.train()
    
    # ============================================================
    # ëª¨ë¸ ì €ì¥
    # ============================================================
    final_model_path = os.path.join(output_dir, "final_model")
    model.save_pretrained(final_model_path)
    tokenizer.save_pretrained(final_model_path)
    print(f"\nâœ… ìµœì¢… ëª¨ë¸ ì €ì¥: {final_model_path}")
    
    # í•™ìŠµ ì™„ë£Œ ì •ë³´ ì €ì¥
    experiment_info['end_time'] = datetime.now().isoformat()
    experiment_info['final_train_loss'] = trainer_stats.training_loss
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(experiment_info, f, indent=2, ensure_ascii=False)
    
    print("\n" + "=" * 70)
    print(f"ğŸ‰ í•™ìŠµ ì™„ë£Œ: {experiment_name}")
    print(f"ğŸ“Š CSV ë¡œê·¸: {csv_log_path}")
    print("=" * 70)
    
    return csv_log_path


# ============================================================
# ë©”ì¸
# ============================================================
def main():
    parser = argparse.ArgumentParser(
        description="Kanana Tool Calling LoRA Training",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # ëª¨ë¸ ì„¤ì •
    parser.add_argument(
        "--model", type=str,
        default="kakaocorp/kanana-1.5-2.1b-instruct-2505",
        help="HuggingFace ëª¨ë¸ ID ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ"
    )
    
    # LoRA ì„¤ì •
    parser.add_argument(
        "--rank", "-r", type=int, default=16,
        help="LoRA rank (alphaëŠ” ìë™ìœ¼ë¡œ rank*2ë¡œ ì„¤ì •ë¨)"
    )
    
    # í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    parser.add_argument("--lr", type=float, default=2e-4, help="Learning rate")
    parser.add_argument("--epochs", type=int, default=3, help="í•™ìŠµ ì—í­ ìˆ˜")
    parser.add_argument("--batch_size", type=int, default=4, help="ë°°ì¹˜ ì‚¬ì´ì¦ˆ")
    parser.add_argument("--grad_accum", type=int, default=4, help="Gradient accumulation steps")
    parser.add_argument("--max_seq_length", type=int, default=9000, help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´")
    parser.add_argument("--weight_decay", type=float, default=0.01, help="Weight decay")
    parser.add_argument("--warmup_ratio", type=float, default=0.03, help="Warmup ratio")
    
    # ë¡œê¹…/ì €ì¥ ì„¤ì •
    parser.add_argument("--logging_steps", type=int, default=10, help="ë¡œê¹… ìŠ¤í… ê°„ê²©")
    parser.add_argument("--eval_steps", type=int, default=50, help="í‰ê°€ ìŠ¤í… ê°„ê²©")
    parser.add_argument("--save_steps", type=int, default=200, help="ì €ì¥ ìŠ¤í… ê°„ê²©")
    
    # ë°ì´í„° ì„¤ì •
    parser.add_argument(
        "--local_data", action="store_true",
        help="ë¡œì»¬ íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ (ê¸°ë³¸: HuggingFace Hubì—ì„œ ë¡œë“œ)"
    )
    parser.add_argument(
        "--hf_datasets", nargs="+",
        default=[
            "NotoriousH2/instructkr-toolflow",
            "NotoriousH2/instructkr-when2call",
            "NotoriousH2/instructkr-apigen"
        ],
        help="HuggingFace Hub ë°ì´í„°ì…‹ IDë“¤"
    )
    parser.add_argument(
        "--data_files", nargs="+",
        default=[
            "sft_when2call_korean.json",
            "sft_synth_helpdesk.json",
            "sft_apigen_mt_5k_korean.json"
        ],
        help="ë¡œì»¬ í•™ìŠµ ë°ì´í„° íŒŒì¼ ê²½ë¡œë“¤ (--local_data ì‚¬ìš© ì‹œ)"
    )
    
    # ì¶œë ¥ ê²½ë¡œ
    parser.add_argument(
        "--output_dir", type=str, default="./experiments",
        help="ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬"
    )
    
    # ê¸°íƒ€
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    
    args = parser.parse_args()
    
    # GPU ì •ë³´ ì¶œë ¥
    print("=" * 70)
    print("ğŸ–¥ï¸ ì‹œìŠ¤í…œ ì •ë³´")
    print("=" * 70)
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    print("=" * 70 + "\n")
    
    # í•™ìŠµ ì‹¤í–‰
    train(args)


if __name__ == "__main__":
    main()

